from src.data_ingestion.twitter_db import TwitterDB
from src.data_ingestion.llm_db import LLMDB
from src.data_ingestion.main_db import MainDB
from src.data_preprocessing.processor import DataProcessor
from src.data_ingestion.twitter_scrape import scrape_user_tweets, scrape_keyword_tweets


def demo_prepare_data():

    llm_sample = [
        {'text_id': '101', 'text': 'This content was generated by GPT-4.',
            'label': 1, 'model': 'gpt-4', 'prompt_name': 'persuade_prompt'},
        {'text_id': '102', 'text': 'Another AI generated essay.',
            'label': 1, 'model': 'gpt-3.5', 'prompt_name': 'inform_prompt'}
    ]

    records = scrape_user_tweets("NASA", max_results=100)
    db = TwitterDB(records)
    twitter_path = db.process_twitter_dataset()
    # db.save_to_csv("./datalake/curated/twitter/tweets.csv")
    llm_path = LLMDB(llm_sample).process_llm_dataset()
    main_path = MainDB(twitter_path, llm_path).merge_to_main()
    return main_path


if __name__ == '__main__':
    main_path = demo_prepare_data()
    processor = DataProcessor(main_path)
    processor.clean_data()
    # processed_path = main_path.parent / f'processed_{main_path.name}'
    data = processor.get_data()
    print(data.head(10))
