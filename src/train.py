# ===== Standard Library =====
import os
import argparse

# ===== Third-Party Libraries =====
import torch
import pandas as pd
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer

# ===== Local Modules =====
from src.data_ingestion.twitter_db import TwitterDB
from src.data_ingestion.llm_db import LLMDB
from src.data_ingestion.main_db import MainDB
from src.data_ingestion.twitter_scrape import scrape_user_tweets, scrape_keyword_tweets
from src.data_preprocessing.processor import DataProcessor
from src.model.lstm import MyLSTM
from src.model.rnn import MyRNN
from src.model.bert import BertClassifier
from src.dataloader.bertdataset import BertDataset
from src.trainer.trainer import Trainer
from src.evaluator.evaluator import Evaluator
from src.utils.convert_indices import convert_indices
from src.utils.seed import set_all_seeds


def demo_prepare_data():

    llm_sample = [
        {'text_id': '101', 'text': 'This content was generated by GPT-4.',
            'label': 1, 'model': 'gpt-4', 'prompt_name': 'persuade_prompt'},
        {'text_id': '102', 'text': 'Another AI generated essay.',
            'label': 1, 'model': 'gpt-3.5', 'prompt_name': 'inform_prompt'}
    ]

    records = scrape_user_tweets("NASA", max_results=100)
    db = TwitterDB(records)
    twitter_path = db.process_twitter_dataset()
    # db.save_to_csv("./datalake/curated/twitter/tweets.csv")
    llm_path = LLMDB(llm_sample).process_llm_dataset()
    main_path = MainDB(twitter_path, llm_path).merge_to_main()
    return main_path


if __name__ == '__main__':
    # main_path = demo_prepare_data()
    # processor = DataProcessor(main_path)
    # processor.clean_data()
    # # processed_path = main_path.parent / f'processed_{main_path.name}'
    # data = processor.get_data()
    # print(data.head(10))
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "model", choices=["rnn", "lstm",'bert'], help="Model type: rnn or lstm")
    parser.add_argument("--epochs", type=int, default=100,
                        help="Number of training epochs")
    parser.add_argument("--learning_rate", type=float,
                        default=0.0001, help="Learning rate")
    parser.add_argument(
        "--output_path", default='./model_save/', help="model save path")
    parser.add_argument(
        "--batch_size", type=int, default=314, help="model batch size")
    args = parser.parse_args()
    # Create output directory if it doesn't exist
    os.makedirs(args.output_path, exist_ok=True)
    print(f"Model will be saved to: {args.output_path}")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    set_all_seeds()
    human_token = pd.read_csv("src/human_token.csv", index_col=0)
    ai_token = pd.read_csv("src/ai_token.csv", index_col=0)
    model_w2v = Word2Vec.load("src/w2vmodel.model")

    # Combine human token and ai token
    token = pd.concat([human_token, ai_token], ignore_index=True)

    # Shuffle and split the data
    X_train, X_temp, y_train, y_temp = train_test_split(
        token["text"], token["human_wrote"], test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42)
    
    # Reset indices to ensure continuous indexing for DataLoader
    X_train = X_train.reset_index(drop=True)
    y_train = y_train.reset_index(drop=True)
    X_val = X_val.reset_index(drop=True)
    y_val = y_val.reset_index(drop=True)
    X_test = X_test.reset_index(drop=True)
    y_test = y_test.reset_index(drop=True)
    train_data = list(zip(X_train, y_train))
    val_data = list(zip(X_val, y_val))
    test_data = list(zip(X_test, y_test))
   # Create model based on argument
    if args.model == "rnn":
        # Convert to indices
        train_data_indices = convert_indices(train_data, model_w2v)
        val_data_indices = convert_indices(val_data, model_w2v)
        test_data_indices = convert_indices(test_data, model_w2v)
        model = MyRNN(model_w2v, hidden_size=300, num_classes=2).to(device)
        trainer = Trainer(device, model, train_data_indices, val_data_indices,
                      learning_rate=args.learning_rate, num_epochs=args.epochs, model_save_dir=args.output_path,batch_size=args.batch_size)
        train_loss, train_acc, val_acc = trainer.train_model()
        test_evaluator = Evaluator(model, test_data_indices, device)
        acc = test_evaluator.accuracy(args.batch_size)
    elif args.model == "lstm":
        train_data_indices = convert_indices(train_data, model_w2v)
        val_data_indices = convert_indices(val_data, model_w2v)
        test_data_indices = convert_indices(test_data, model_w2v)
        model = MyLSTM(model_w2v, hidden_size=256, num_classes=2).to(device)
        trainer = Trainer(device, model, train_data_indices, val_data_indices,
                      learning_rate=args.learning_rate, num_epochs=args.epochs, model_save_dir=args.output_path,batch_size=args.batch_size)
        train_loss, train_acc, val_acc = trainer.train_model()
        test_evaluator = Evaluator(model, test_data_indices, device)
        acc = test_evaluator.accuracy(args.batch_size)
    elif args.model == "bert":
        model = BertClassifier(num_labels=2, dropout=0.3, freeze_bert=False).to(device)
        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        train_dataset = BertDataset(X_train, y_train, tokenizer)
        val_dataset = BertDataset(X_val, y_val, tokenizer)
        test_dataset = BertDataset(X_test, y_test, tokenizer)
        trainer = Trainer(device, model, train_dataset, val_dataset,
                      learning_rate=args.learning_rate, num_epochs=args.epochs, model_save_dir=args.output_path,batch_size=args.batch_size)
        train_loss, train_acc, val_acc = trainer.train_model()
        
        test_evaluator = Evaluator(model, test_dataset, device)
        acc = test_evaluator.accuracy(args.batch_size)

    print(f"test accuracy: {acc}", flush=True)
